# src/cli.py
"""
–ï–¥–∏–Ω—ã–π CLI-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã
"""
import click
import json
from pathlib import Path
import sys

# –ò–º–ø–æ—Ä—Ç –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
from src.transcribe import VideoTranscriber
from src.segment import TranscriptSegmenter
from src.summarize import SegmentSummarizer
from src.meta_analysis import MetaAnalyzer
from src.extract_terms import TermExtractor
from src.generate_questions import QuestionGenerator
from src.search_articles import ArticleSearcher
from src.export import ReportExporter


@click.group()
@click.version_option(version="1.0.0")
def cli():
    """
    –°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ

    –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ –∏–ª–∏ 'process-all' –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞.
    """
    pass


@cli.command()
@click.argument('video', type=str)
@click.option('--model', default='base', help='–ú–æ–¥–µ–ª—å Whisper (tiny/base/small/medium/large)')
@click.option('--language', default='ru', help='–Ø–∑—ã–∫ –≤–∏–¥–µ–æ')
@click.option('--device', default='auto', help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cuda/cpu/auto)')
@click.option('--output-dir', default='artifacts', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
def transcribe(video, model, language, device, output_dir):
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –≤–∏–¥–µ–æ"""
    click.echo(f"üé¨ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –≤–∏–¥–µ–æ: {video}")

    transcriber = VideoTranscriber(
        model_size=model,
        device=device,
        output_dir=output_dir
    )

    output_path = transcriber.process_video(video, language)
    click.echo(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_path}")


@cli.command()
@click.argument('transcript', type=click.Path(exists=True))
@click.option('--method', default='similarity', type=click.Choice(['similarity', 'clustering']))
@click.option('--threshold', default=0.7, type=float, help='–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏')
@click.option('--device', default='auto', help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ')
def segment(transcript, method, threshold, device):
    """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏"""
    click.echo(f"üìä –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {transcript}")

    transcript_path = Path(transcript)
    segmenter = TranscriptSegmenter(device=device)

    segments = segmenter.create_semantic_segments(
        transcript_path,
        method=method,
        threshold=threshold
    )

    segmenter.save_segments(segments, transcript_path.parent)
    click.echo(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(segments['segments'])} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")


@cli.command()
@click.argument('segments', type=click.Path(exists=True))
@click.option('--model', default='cointegrated/rut5-base-absum', help='–ú–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏')
@click.option('--device', default='auto', help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ')
def summarize(segments, model, device):
    """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
    click.echo(f"üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {segments}")

    segments_path = Path(segments)
    summarizer = SegmentSummarizer(
        model_name=model,
        device=device
    )

    summaries = summarizer.process_segments_file(segments_path)
    click.echo(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(summaries['segments'])} —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π")


@cli.command()
@click.argument('summaries', type=click.Path(exists=True))
@click.option('--use-llm', is_flag=True, help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM (GigaChat) –¥–ª—è overview –∏ key points (Phase 2)')
@click.option('--use-keybert', is_flag=True, help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å KeyBERT –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (Phase 2)')
def meta_analyze(summaries, use_llm, use_keybert):
    """–ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑ –∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã"""
    click.echo(f"üîç –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑: {summaries}")

    if use_llm:
        click.echo("   ü§ñ –ò—Å–ø–æ–ª—å–∑—É—é LLM (GigaChat) –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    if use_keybert:
        click.echo("   üîë –ò—Å–ø–æ–ª—å–∑—É—é KeyBERT –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")

    summaries_path = Path(summaries)
    analyzer = MetaAnalyzer(use_llm=use_llm, use_keybert=use_keybert)

    analysis = analyzer.process_summaries_file(summaries_path)
    click.echo(f"‚úÖ –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω")


@cli.command()
@click.argument('transcript', type=click.Path(exists=True))
@click.option('--model', default='ru_core_news_lg', help='SpaCy –º–æ–¥–µ–ª—å')
def extract_terms(transcript, model):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    click.echo(f"üìö –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤: {transcript}")

    transcript_path = Path(transcript)
    extractor = TermExtractor(model_name=model)

    results = extractor.process_transcript(transcript_path)
    click.echo(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {results['statistics']['total_terms']} —Ç–µ—Ä–º–∏–Ω–æ–≤")


@cli.command()
@click.argument('summaries', type=click.Path(exists=True))
@click.option('--num-questions', default=20, type=int, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤')
@click.option('--use-model', is_flag=True, help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å T5 –º–æ–¥–µ–ª—å')
@click.option('--use-llm', is_flag=True, help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM (GigaChat) –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (Phase 2)')
def generate_questions(summaries, num_questions, use_model, use_llm):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤"""
    click.echo(f"‚ùì –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤: {summaries}")

    if use_llm:
        click.echo("   ü§ñ –ò—Å–ø–æ–ª—å–∑—É—é LLM (GigaChat) –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")

    summaries_path = Path(summaries)
    generator = QuestionGenerator(use_model=use_model, use_llm=use_llm)

    questions = generator.process_summaries_file(summaries_path, num_questions)
    click.echo(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {questions['total_questions']} –≤–æ–ø—Ä–æ—Å–æ–≤")


@cli.command()
@click.argument('terms', type=click.Path(exists=True))
@click.option('--enable-scraping', is_flag=True, help='–í–∫–ª—é—á–∏—Ç—å –≤–µ–±-—Å–∫—Ä–µ–π–ø–∏–Ω–≥')
@click.option('--max-articles', default=10, type=int, help='–ú–∞–∫—Å–∏–º—É–º —Å—Ç–∞—Ç–µ–π')
def search_articles(terms, enable_scraping, max_articles):
    """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
    click.echo(f"üîé –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π: {terms}")

    if not enable_scraping:
        click.echo("‚ö†Ô∏è  –í–µ–±-—Å–∫—Ä–µ–π–ø–∏–Ω–≥ –æ—Ç–∫–ª—é—á–µ–Ω (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --enable-scraping)")

    terms_path = Path(terms)
    searcher = ArticleSearcher(
        enable_scraping=enable_scraping,
        max_articles=max_articles
    )

    articles = searcher.process_terms_file(terms_path)
    click.echo(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {articles['total_articles']} —Å—Ç–∞—Ç–µ–π")


@cli.command()
@click.argument('artifacts_dir', type=click.Path(exists=True))
@click.option('--no-pdf', is_flag=True, help='–ù–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å PDF')
def export_report(artifacts_dir, no_pdf):
    """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞ (HTML/PDF)"""
    click.echo(f"üìÑ –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞: {artifacts_dir}")

    artifacts_path = Path(artifacts_dir)
    exporter = ReportExporter()

    results = exporter.export_all(artifacts_path)

    click.echo(f"‚úÖ HTML: {results.get('html', 'N/A')}")
    if 'pdf' in results and not no_pdf:
        click.echo(f"‚úÖ PDF: {results['pdf']}")


@cli.command()
@click.argument('video', type=str)
@click.option('--model', default='base', help='–ú–æ–¥–µ–ª—å Whisper')
@click.option('--language', default='ru', help='–Ø–∑—ã–∫')
@click.option('--device', default='auto', help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ')
@click.option('--output-dir', default='artifacts', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
@click.option('--enable-scraping', is_flag=True, help='–í–∫–ª—é—á–∏—Ç—å –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π')
@click.option('--skip-questions', is_flag=True, help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–æ–ø—Ä–æ—Å–æ–≤')
@click.option('--skip-articles', is_flag=True, help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π')
@click.option('--use-llm', is_flag=True, help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM (GigaChat) –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (Phase 2)')
@click.option('--use-keybert', is_flag=True, help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å KeyBERT –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (Phase 2)')
def process_all(video, model, language, device, output_dir, enable_scraping, skip_questions, skip_articles, use_llm, use_keybert):
    """
    –ü–û–õ–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù: —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è ‚Üí —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è ‚Üí —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è ‚Üí
    –∞–Ω–∞–ª–∏–∑ ‚Üí —Ç–µ—Ä–º–∏–Ω—ã ‚Üí –≤–æ–ø—Ä–æ—Å—ã ‚Üí —Å—Ç–∞—Ç—å–∏ ‚Üí —ç–∫—Å–ø–æ—Ä—Ç
    """
    click.echo("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ\n")
    click.echo("=" * 60)

    try:
        # –≠—Ç–∞–ø 1: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
        click.echo("\n[1/8] üé¨ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è...")
        transcriber = VideoTranscriber(model_size=model, device=device, output_dir=output_dir)
        output_path = transcriber.process_video(video, language)
        transcript_path = output_path / "transcript_raw.json"

        # –≠—Ç–∞–ø 2: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        click.echo("\n[2/8] üìä –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è...")
        segmenter = TranscriptSegmenter(device=device)
        segments = segmenter.create_semantic_segments(transcript_path)
        segmenter.save_segments(segments, output_path)
        segments_path = output_path / "segments_semantic.json"

        # –≠—Ç–∞–ø 3: –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        click.echo("\n[3/8] üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è...")
        summarizer = SegmentSummarizer(device=device)
        summaries = summarizer.process_segments_file(segments_path)
        summaries_path = output_path / "summaries_per_segment.json"

        # –≠—Ç–∞–ø 4: –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑
        click.echo("\n[4/8] üîç –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑...")
        if use_llm:
            click.echo("   ü§ñ –ò—Å–ø–æ–ª—å–∑—É—é LLM (GigaChat) –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
        if use_keybert:
            click.echo("   üîë –ò—Å–ø–æ–ª—å–∑—É—é KeyBERT –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        analyzer = MetaAnalyzer(use_llm=use_llm, use_keybert=use_keybert)
        analysis = analyzer.process_summaries_file(summaries_path)

        # –≠—Ç–∞–ø 5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤
        click.echo("\n[5/8] üìö –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤...")
        extractor = TermExtractor()
        terms = extractor.process_transcript(transcript_path)
        terms_path = output_path / "terms_and_entities.json"

        # –≠—Ç–∞–ø 6: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if not skip_questions:
            click.echo("\n[6/8] ‚ùì –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤...")
            if use_llm:
                click.echo("   ü§ñ –ò—Å–ø–æ–ª—å–∑—É—é LLM (GigaChat) –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
            generator = QuestionGenerator(use_model=False, use_llm=use_llm)
            questions = generator.process_summaries_file(summaries_path)
        else:
            click.echo("\n[6/8] ‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–æ–ø—Ä–æ—Å–æ–≤")

        # –≠—Ç–∞–ø 7: –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if not skip_articles:
            click.echo("\n[7/8] üîé –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π...")
            searcher = ArticleSearcher(enable_scraping=enable_scraping)
            articles = searcher.process_terms_file(terms_path)
        else:
            click.echo("\n[7/8] ‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π")

        # –≠—Ç–∞–ø 8: –≠–∫—Å–ø–æ—Ä—Ç
        click.echo("\n[8/8] üìÑ –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞...")
        exporter = ReportExporter()
        results = exporter.export_all(output_path)

        # –§–∏–Ω–∞–ª
        click.echo("\n" + "=" * 60)
        click.echo("‚úÖ –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–Å–ù –£–°–ü–ï–®–ù–û!")
        click.echo("=" * 60)
        click.echo(f"\nüìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
        click.echo(f"üåê HTML –æ—Ç—á—ë—Ç: {results.get('html', 'N/A')}")
        if 'pdf' in results:
            click.echo(f"üìÑ PDF –æ—Ç—á—ë—Ç: {results['pdf']}")

        # –ö—Ä–∞—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        with open(output_path / "final_summary.json", 'r', encoding='utf-8') as f:
            final = json.load(f)

        stats = final.get('statistics', {})
        click.echo(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        click.echo(f"   ‚Ä¢ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {stats.get('total_duration_seconds', 0) / 60:.1f} –º–∏–Ω—É—Ç")
        click.echo(f"   ‚Ä¢ –°–µ–≥–º–µ–Ω—Ç–æ–≤: {stats.get('num_segments', 0)}")
        click.echo(f"   ‚Ä¢ –°–ª–æ–≤: {stats.get('total_words', 0)}")

        return 0

    except Exception as e:
        click.echo(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}", err=True)
        import traceback
        traceback.print_exc()
        return 1


@cli.command()
@click.argument('artifacts_dir', type=click.Path(exists=True))
def status(artifacts_dir):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    artifacts_path = Path(artifacts_dir)
    checkpoint_path = artifacts_path / "checkpoint.json"

    if not checkpoint_path.exists():
        click.echo("‚ùå Checkpoint –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    with open(checkpoint_path, 'r', encoding='utf-8') as f:
        checkpoint = json.load(f)

    click.echo(f"üìç –°—Ç–∞—Ç—É—Å: {checkpoint.get('stage', 'unknown')}")
    click.echo(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {checkpoint.get('output_path', 'N/A')}")

    files = checkpoint.get('files', {})
    click.echo("\nüìÑ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    for name, path in files.items():
        exists = "‚úÖ" if Path(path).exists() else "‚ùå"
        click.echo(f"   {exists} {name}: {Path(path).name}")


if __name__ == '__main__':
    sys.exit(cli())